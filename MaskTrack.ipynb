{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H7b4cJRdr5w"
   },
   "source": [
    "# MaskTrack - ResnetUNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Format data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "q9s0Xp1edr54"
   },
   "source": [
    "DAVIS2017-MASTER\n",
    " |- JPEGImages \n",
    " |- Annotations\n",
    " |- ImageSets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 1: Put train images in JPEGImages folder\n",
    "Step 2: Put train annotated images in Annotations folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZpOwcvo0dr54"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "\n",
    "im_dir = 'D:/INTERVIEWS/stryker/Pytorch-UNet/data/DAVIS2017-master/JPEGImages'\n",
    "an_dir = 'D:/INTERVIEWS/stryker/Pytorch-UNet/data/DAVIS2017-master/Annotations'\n",
    "\n",
    "im_files = glob.glob(im_dir + '/**/*.jpg', recursive=True)\n",
    "an_files = glob.glob(an_dir + '/**/*.png', recursive=True)\n",
    "\n",
    "im_files = [i.split('/DAVIS2017-master/')[1] for i in im_files]\n",
    "an_files = [i.split('/DAVIS2017-master/')[1] for i in an_files]\n",
    "\n",
    "conc = [str('%s %s\\n') %(i,j) for i,j in zip(im_files, an_files)]\n",
    "\n",
    "random.shuffle(conc)\n",
    "\n",
    "tr_len = int(0.80 * len(conc))\n",
    "va_len = len(conc) - int(0.80 * len(conc))\n",
    "\n",
    "file1 = open(\"train.txt\",\"w\")#write mode \n",
    "file1.writelines(conc[:tr_len]) \n",
    "file1.close()\n",
    "\n",
    "file2 = open(\"val.txt\",\"w\")#write mode \n",
    "file2.writelines(conc[tr_len:]) \n",
    "file2.close()\n",
    "\n",
    "file3 = open(\"trainval.txt\",\"w\")#write mode \n",
    "file3.writelines(conc) \n",
    "file3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation - Augmented Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import cv2 \n",
    "from PIL import Image\n",
    "import random\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "class Masktrack_aug():\n",
    "    def __init__(self,Davis_path=None):\n",
    "        self.base_path = Davis_path\n",
    "        self.annotation_path = os.path.join(self.base_path,'Annotations/')\n",
    "        self.deformation_path = os.path.join(self.base_path,'Deformations/')\n",
    "        self.gt_path = os.path.join(self.base_path,'Annotations_binary/')\n",
    "        if not os.path.exists(self.deformation_path):\n",
    "            os.makedirs(self.deformation_path)\n",
    "        if not os.path.exists(self.gt_path):\n",
    "            os.makedirs(self.gt_path)\n",
    "        with open(os.path.join(self.base_path,'ImageSets','train.txt')) as f:\n",
    "            trainset = [v.split(' ')[0].split('JPEGImages\\\\')[1].replace('jpg','png') for v in f.readlines()]\n",
    "        with open(os.path.join(self.base_path,'ImageSets','val.txt')) as f:\n",
    "            valset = [v.split(' ')[0].split('JPEGImages\\\\')[1].replace('jpg','png') for v in f.readlines()]  \n",
    "        self.videos = trainset+valset  \n",
    "        \n",
    "        #self.videos = os.listdir(self.annotation_path)\n",
    "        print('total {} videos'.format(len(self.videos)))\n",
    "        \n",
    "    def augment_image_and_mask(self,gt_arr,gt_path=None,affine_transformation_path=None, non_rigid_deform_path=None):\n",
    "        #gt_arr shape (H,W) and binary(0,1)\n",
    "\n",
    "        # let us do non-rigid deformation\n",
    "        N = 5\n",
    "        Delta = 0.05\n",
    "        H,W = gt_arr.shape\n",
    "        #get the target boundary\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "        boundary = cv2.dilate(gt_arr, kernel)-gt_arr\n",
    "        boundindex = np.where(boundary==1)\n",
    "        num_index = boundindex[0].shape[0]\n",
    "        if num_index>N:\n",
    "            maxH,minH = max(boundindex[0]),min(boundindex[0])\n",
    "            tarH = maxH - minH\n",
    "            maxW,minW = max(boundindex[1]),min(boundindex[1])\n",
    "            tarW = maxW - minW\n",
    "\n",
    "            # thin plate spline coord num    \n",
    "            randindex = [random.randint(0,num_index-1) for _ in range(N)]\n",
    "            sourcepoints=[]\n",
    "            targetpoints = []\n",
    "            for i in range(N):\n",
    "                sourcepoints.append((boundindex[1][randindex[i]],boundindex[0][randindex[i]]))\n",
    "                x = boundindex[1][randindex[i]]+int(random.uniform(-Delta,Delta)*tarW)\n",
    "                y = boundindex[0][randindex[i]]+int(random.uniform(-Delta,Delta)*tarH)\n",
    "                targetpoints.append((x,y))\n",
    "        \n",
    "            sourceshape = np.array(sourcepoints,np.int32)\n",
    "            sourceshape=sourceshape.reshape(1,-1,2)\n",
    "            targetshape = np.array(targetpoints,np.int32)\n",
    "            targetshape=targetshape.reshape(1,-1,2)\n",
    "\n",
    "            matches =[]\n",
    "            for i in range(0,N):\n",
    "                matches.append(cv2.DMatch(i,i,0))\n",
    "            tps= cv2.createThinPlateSplineShapeTransformer()\n",
    "            tps.estimateTransformation(targetshape, sourceshape,matches)\n",
    "            no_grid_img=tps.warpImage(gt_arr)\n",
    "            kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "            no_grid_img = cv2.dilate(no_grid_img,kernel)\n",
    "            gt_out = gt_arr*255\n",
    "            no_grid_img = no_grid_img*255\n",
    "            gt_out = Image.fromarray(gt_out)\n",
    "            no_grid_out = Image.fromarray(no_grid_img)\n",
    "\n",
    "            scale=0.98\n",
    "            randScale = random.uniform(scale,1/scale)\n",
    "            M = cv2.getRotationMatrix2D(((maxH+minH)*0.5, (maxW-minW)*0.5), 0, randScale)\n",
    "            \n",
    "            dx = round(random.uniform(-0.05,0.05)*tarW)\n",
    "            dy = round(random.uniform(-0.05,0.05)*tarH)\n",
    "            M[0,2]+=dx\n",
    "            M[1,2]+=dy\n",
    "            affine_out = cv2.warpAffine(gt_arr, M, (W, H))*255\n",
    "            affine_out = Image.fromarray(affine_out)\n",
    "        else:\n",
    "            gt_out = Image.fromarray(gt_arr*255)\n",
    "            no_grid_out = Image.fromarray(gt_arr*255)\n",
    "            affine_out = Image.fromarray(gt_arr*255)\n",
    "\n",
    "        gt_out.save(gt_path)\n",
    "        no_grid_out.save(non_rigid_deform_path)\n",
    "        affine_out.save(affine_transformation_path)\n",
    "\n",
    "    def script(self):\n",
    "        mask_gt_path = self.annotation_path\n",
    "\n",
    "        frames = self.videos\n",
    "        \n",
    "        no_objects = 1\n",
    "        for k,frame in enumerate(frames):\n",
    "            frame_path = os.path.join(mask_gt_path,frame)\n",
    "            frame_gt_image = Image.open(frame_path)\n",
    "            frame_gt_image = np.array(frame_gt_image)\n",
    "            frame_index = frame.split('\\\\')[1][:-4]\n",
    "            video = frame.split('\\\\')[0]\n",
    "\n",
    "            label_folder_path = os.path.join(self.gt_path,video)\n",
    "            deform_folder_path = os.path.join(self.deformation_path,video)\n",
    "            if not os.path.exists(label_folder_path):\n",
    "                os.makedirs(label_folder_path)\n",
    "\n",
    "            if not os.path.exists(deform_folder_path):\n",
    "                os.makedirs(deform_folder_path)\n",
    "\n",
    "            for object_id in range(1,no_objects+1):\n",
    "                temp = frame_gt_image.copy()\n",
    "                \n",
    "                m1 = temp==object_id\n",
    "                m0 = temp!=object_id\n",
    "                \n",
    "                temp[m1]=1\n",
    "                temp[m0]=0\n",
    "                \n",
    "                gt_path = os.path.join(label_folder_path,frame_index+'_'+str(object_id)+'.png')\n",
    "                aff_path = os.path.join(deform_folder_path,frame_index+'_'+str(object_id)+'_d1.png')\n",
    "                non_path = os.path.join(deform_folder_path,frame_index+'_'+str(object_id)+'_d2.png')\n",
    "                self.augment_image_and_mask(temp,gt_path=gt_path,\n",
    "                                            affine_transformation_path=aff_path,\n",
    "                                            non_rigid_deform_path=non_path)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Folder Structure - Generated Coarsened and Deformed Masks"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "DAVIS2017-MASTER\n",
    " |- JPEGImages \n",
    " |- Annotations\n",
    " |- ImageSets\n",
    " |- Annotations_binary\n",
    " |- Deformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Base Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet 18 + Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, num_input_channel, n_class):\n",
    "        super().__init__()\n",
    "        self.num_input_channel = num_input_channel\n",
    "        self.base_model = models.resnet18(pretrained=True)\n",
    "        self.base_model.conv1 = nn.Conv2d(num_input_channel, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        self.base_layers = list(self.base_model.children())\n",
    "\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3]) # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1_1x1 = convrelu(64, 64, 1, 0)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2_1x1 = convrelu(128, 128, 1, 0)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3_1x1 = convrelu(256, 256, 1, 0)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4_1x1 = convrelu(512, 512, 1, 0)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)\n",
    "        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)\n",
    "        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)\n",
    "        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(num_input_channel, 64, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(64, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x_original = self.conv_original_size0(input)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0 = self.layer0(input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IgzrKDq2dr57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           2,368\n",
      "              ReLU-2         [-1, 64, 224, 224]               0\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "              ReLU-4         [-1, 64, 224, 224]               0\n",
      "            Conv2d-5         [-1, 64, 112, 112]          12,544\n",
      "            Conv2d-6         [-1, 64, 112, 112]          12,544\n",
      "       BatchNorm2d-7         [-1, 64, 112, 112]             128\n",
      "       BatchNorm2d-8         [-1, 64, 112, 112]             128\n",
      "              ReLU-9         [-1, 64, 112, 112]               0\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-11           [-1, 64, 56, 56]               0\n",
      "        MaxPool2d-12           [-1, 64, 56, 56]               0\n",
      "           Conv2d-13           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-14           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-15           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "             ReLU-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "             ReLU-24           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-25           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-26           [-1, 64, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-28           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-29           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-30           [-1, 64, 56, 56]             128\n",
      "             ReLU-31           [-1, 64, 56, 56]               0\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33           [-1, 64, 56, 56]          36,864\n",
      "           Conv2d-34           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-35           [-1, 64, 56, 56]             128\n",
      "      BatchNorm2d-36           [-1, 64, 56, 56]             128\n",
      "             ReLU-37           [-1, 64, 56, 56]               0\n",
      "             ReLU-38           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-39           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-40           [-1, 64, 56, 56]               0\n",
      "           Conv2d-41          [-1, 128, 28, 28]          73,728\n",
      "           Conv2d-42          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-43          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "             ReLU-46          [-1, 128, 28, 28]               0\n",
      "           Conv2d-47          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-48          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "           Conv2d-51          [-1, 128, 28, 28]           8,192\n",
      "           Conv2d-52          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-54          [-1, 128, 28, 28]             256\n",
      "             ReLU-55          [-1, 128, 28, 28]               0\n",
      "             ReLU-56          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-57          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-58          [-1, 128, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-60          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-62          [-1, 128, 28, 28]             256\n",
      "             ReLU-63          [-1, 128, 28, 28]               0\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 128, 28, 28]         147,456\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "      BatchNorm2d-68          [-1, 128, 28, 28]             256\n",
      "             ReLU-69          [-1, 128, 28, 28]               0\n",
      "             ReLU-70          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-71          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-72          [-1, 128, 28, 28]               0\n",
      "           Conv2d-73          [-1, 256, 14, 14]         294,912\n",
      "           Conv2d-74          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-75          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-76          [-1, 256, 14, 14]             512\n",
      "             ReLU-77          [-1, 256, 14, 14]               0\n",
      "             ReLU-78          [-1, 256, 14, 14]               0\n",
      "           Conv2d-79          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-80          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-82          [-1, 256, 14, 14]             512\n",
      "           Conv2d-83          [-1, 256, 14, 14]          32,768\n",
      "           Conv2d-84          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-85          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-86          [-1, 256, 14, 14]             512\n",
      "             ReLU-87          [-1, 256, 14, 14]               0\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-89          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-90          [-1, 256, 14, 14]               0\n",
      "           Conv2d-91          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-92          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-93          [-1, 256, 14, 14]             512\n",
      "      BatchNorm2d-94          [-1, 256, 14, 14]             512\n",
      "             ReLU-95          [-1, 256, 14, 14]               0\n",
      "             ReLU-96          [-1, 256, 14, 14]               0\n",
      "           Conv2d-97          [-1, 256, 14, 14]         589,824\n",
      "           Conv2d-98          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 14, 14]             512\n",
      "     BatchNorm2d-100          [-1, 256, 14, 14]             512\n",
      "            ReLU-101          [-1, 256, 14, 14]               0\n",
      "            ReLU-102          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-103          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-104          [-1, 256, 14, 14]               0\n",
      "          Conv2d-105            [-1, 512, 7, 7]       1,179,648\n",
      "          Conv2d-106            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-107            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-108            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-109            [-1, 512, 7, 7]               0\n",
      "            ReLU-110            [-1, 512, 7, 7]               0\n",
      "          Conv2d-111            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-114            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-115            [-1, 512, 7, 7]         131,072\n",
      "          Conv2d-116            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-118            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-119            [-1, 512, 7, 7]               0\n",
      "            ReLU-120            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-121            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-122            [-1, 512, 7, 7]               0\n",
      "          Conv2d-123            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-124            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-125            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-126            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-127            [-1, 512, 7, 7]               0\n",
      "            ReLU-128            [-1, 512, 7, 7]               0\n",
      "          Conv2d-129            [-1, 512, 7, 7]       2,359,296\n",
      "          Conv2d-130            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-131            [-1, 512, 7, 7]           1,024\n",
      "     BatchNorm2d-132            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-133            [-1, 512, 7, 7]               0\n",
      "            ReLU-134            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-135            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-136            [-1, 512, 7, 7]               0\n",
      "          Conv2d-137            [-1, 512, 7, 7]         262,656\n",
      "            ReLU-138            [-1, 512, 7, 7]               0\n",
      "        Upsample-139          [-1, 512, 14, 14]               0\n",
      "          Conv2d-140          [-1, 256, 14, 14]          65,792\n",
      "            ReLU-141          [-1, 256, 14, 14]               0\n",
      "          Conv2d-142          [-1, 512, 14, 14]       3,539,456\n",
      "            ReLU-143          [-1, 512, 14, 14]               0\n",
      "        Upsample-144          [-1, 512, 28, 28]               0\n",
      "          Conv2d-145          [-1, 128, 28, 28]          16,512\n",
      "            ReLU-146          [-1, 128, 28, 28]               0\n",
      "          Conv2d-147          [-1, 256, 28, 28]       1,474,816\n",
      "            ReLU-148          [-1, 256, 28, 28]               0\n",
      "        Upsample-149          [-1, 256, 56, 56]               0\n",
      "          Conv2d-150           [-1, 64, 56, 56]           4,160\n",
      "            ReLU-151           [-1, 64, 56, 56]               0\n",
      "          Conv2d-152          [-1, 256, 56, 56]         737,536\n",
      "            ReLU-153          [-1, 256, 56, 56]               0\n",
      "        Upsample-154        [-1, 256, 112, 112]               0\n",
      "          Conv2d-155         [-1, 64, 112, 112]           4,160\n",
      "            ReLU-156         [-1, 64, 112, 112]               0\n",
      "          Conv2d-157        [-1, 128, 112, 112]         368,768\n",
      "            ReLU-158        [-1, 128, 112, 112]               0\n",
      "        Upsample-159        [-1, 128, 224, 224]               0\n",
      "          Conv2d-160         [-1, 64, 224, 224]         110,656\n",
      "            ReLU-161         [-1, 64, 224, 224]               0\n",
      "          Conv2d-162          [-1, 2, 224, 224]             130\n",
      "================================================================\n",
      "Total params: 28,983,234\n",
      "Trainable params: 28,983,234\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.77\n",
      "Forward/backward pass size (MB): 416.12\n",
      "Params size (MB): 110.56\n",
      "Estimated Total Size (MB): 527.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "model = ResNetUNet(4,2)\n",
    "summary(model, input_size=(4, 224, 224))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "================================================================\n",
    "Total params: 28,983,234\n",
    "Trainable params: 28,983,234\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.77\n",
    "Forward/backward pass size (MB): 416.12\n",
    "Params size (MB): 110.56\n",
    "Estimated Total Size (MB): 527.45\n",
    "----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8NEjIOogdr57"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0570, 0.0828, 0.0702,  ..., 0.0777, 0.0553, 0.0574],\n",
       "          [0.0873, 0.0657, 0.0686,  ..., 0.0560, 0.0699, 0.0735],\n",
       "          [0.0791, 0.0901, 0.0764,  ..., 0.0738, 0.0609, 0.0802],\n",
       "          ...,\n",
       "          [0.0489, 0.0682, 0.0908,  ..., 0.0823, 0.0701, 0.0622],\n",
       "          [0.0641, 0.0653, 0.0792,  ..., 0.0656, 0.0791, 0.0769],\n",
       "          [0.0620, 0.0727, 0.0715,  ..., 0.0852, 0.0739, 0.0726]],\n",
       "\n",
       "         [[0.1194, 0.1143, 0.1075,  ..., 0.1045, 0.1238, 0.1104],\n",
       "          [0.1138, 0.1106, 0.1166,  ..., 0.0958, 0.1116, 0.1217],\n",
       "          [0.0950, 0.1110, 0.1144,  ..., 0.1291, 0.1236, 0.1331],\n",
       "          ...,\n",
       "          [0.0888, 0.1190, 0.1361,  ..., 0.1296, 0.1068, 0.1052],\n",
       "          [0.1135, 0.1174, 0.1167,  ..., 0.0933, 0.1054, 0.1188],\n",
       "          [0.1053, 0.1256, 0.1232,  ..., 0.1132, 0.1030, 0.1151]]]],\n",
       "       grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity Check, Checking Output on newly defined model\n",
    "\n",
    "trial = np.random.randn(1,4,224,224)\n",
    "model(torch.from_numpy(trial).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset davis17_offline_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "class DAVIS17Offline(Dataset):\n",
    "    def __init__(self, train=True, mini=False, mega=False,\n",
    "                 inputRes=None,\n",
    "                 db_root_dir='DAVIS17',\n",
    "                 transform=None):\n",
    "\n",
    "        self.train = train\n",
    "        self.mini = mini\n",
    "        self.mega = mega\n",
    "        self.inputRes = inputRes\n",
    "        self.db_root_dir = db_root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        if self.mini == False and self.mega == False:\n",
    "            if self.train:\n",
    "                fname = 'train'\n",
    "                # fname = 'train_seqs'\n",
    "            else:\n",
    "                fname = 'val'\n",
    "                # fname = 'val_seqs'\n",
    "        elif self.mini == True:\n",
    "            if self.train:\n",
    "                fname = 'train_mini'\n",
    "                # fname = 'train_seqs'\n",
    "            else:\n",
    "                fname = 'val_mini'\n",
    "                # fname = 'val_seqs'\n",
    "        elif self.mega == True:\n",
    "            if self.train:\n",
    "                fname = 'train_mega'\n",
    "            else:\n",
    "                fname = 'val_mega'\n",
    "\n",
    "        img_list = []\n",
    "        labels = []\n",
    "        deformations = []\n",
    "\n",
    "        # Initialize the original DAVIS splits for training the parent network\n",
    "        with open(os.path.join(db_root_dir, 'ImageSets/' + fname + '.txt')) as f:\n",
    "            seqs = f.readlines()\n",
    "\n",
    "            for seq in seqs:\n",
    "                seq = seq.split(' ')[0]\n",
    "                image = seq\n",
    "                no_objects = 1\n",
    "\n",
    "                # for image in images:\n",
    "                image_id = image.split('\\\\')[-1][:-4]\n",
    "                video = image.split('\\\\')[1]\n",
    "\n",
    "                for object_id in range(1,no_objects+1):\n",
    "                    for df in [1,2]:\n",
    "                        img_list.append(image)\n",
    "                        labels.append(os.path.join('Annotations_binary', video, image_id + '_' + str(object_id) + '.png'))\n",
    "                        deformations.append(os.path.join('Deformations', video, image_id + '_' + str(object_id) + '_d' + str(df) + '.png'))\n",
    "\n",
    "        assert (len(labels) == len(img_list))\n",
    "        assert (len(labels) == len(deformations))\n",
    "\n",
    "        self.img_list = img_list\n",
    "        self.deformations = deformations\n",
    "        self.labels = labels\n",
    "\n",
    "        print('Done initializing ' + fname + ' Dataset')\n",
    "        print(self.__len__(),'images')\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img = cv2.imread(os.path.join(self.db_root_dir, self.img_list[idx]))\n",
    "        label = Image.open(os.path.join(self.db_root_dir, self.labels[idx]))\n",
    "        deformation = Image.open(os.path.join(self.db_root_dir, self.deformations[idx]))\n",
    "        #print(os.path.join(self.db_root_dir, self.deformations[idx]))\n",
    "        img,label,deformation = self.transform(np.array(img), np.array(label), np.array(deformation), self.inputRes)\n",
    "        sample = {'image': img, 'gt': label, 'deformation': deformation}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Offline Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from utility_functions import *\n",
    "from path import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available\n"
     ]
    }
   ],
   "source": [
    "# Setting of parameters\n",
    "NoLabels = 2\n",
    "debug_mode = False\n",
    "use_cuda = False\n",
    "weight_decay = float(0.001)\n",
    "base_lr = float(0.0005)\n",
    "resume_epoch = int(0)  # Default is 0, change if want to resume\n",
    "nEpochs = int(1)  # 1 epoch \n",
    "batch_size = int(1)\n",
    "vbatch_size = 3\n",
    "db_root_dir = 'DAVIS2017-master'\n",
    "nAveGrad = 4  # keep it even\n",
    "\n",
    "save_dir = os.path.join(db_root_dir, 'lr_' + str(base_lr) + '_wd_' + str(weight_decay))\n",
    "\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(os.path.join(save_dir))\n",
    "\n",
    "learnRate = base_lr\n",
    "\n",
    "\"\"\"Initialise the network\"\"\"\n",
    "\n",
    "net = ResNetUNet(4,int(NoLabels))\n",
    "\n",
    "net.float()\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.set_device(0)\n",
    "\n",
    "if use_cuda:\n",
    "    print('use_cuda')\n",
    "    net.cuda()\n",
    "else:\n",
    "    print('CUDA not available')\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(),lr=base_lr, momentum=0.9, weight_decay=weight_decay)\n",
    "\n",
    "if os.path.exists(os.path.join(save_dir, 'logs')) == False:\n",
    "    os.mkdir(os.path.join(save_dir, 'logs'))\n",
    "    \n",
    "file_offline_loss = open(os.path.join(save_dir, 'logs/logs_offline_training_start_epoch_' + str(resume_epoch) + '.txt'), 'w+')\n",
    "file_offline_val_loss = open(os.path.join(save_dir, 'logs/logs_offline_training_val_start_epoch_' + str(resume_epoch) + '.txt'), 'w+')\n",
    "\n",
    "loss_array = []\n",
    "loss_minibatch_array = []\n",
    "precision_train_array  = []\n",
    "recall_train_array = []\n",
    "\n",
    "loss_val_array = []\n",
    "precision_val_array = []\n",
    "recall_val_array = []\n",
    "\n",
    "aveGrad = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing train Dataset\n",
      "9932 images\n",
      "Done initializing val Dataset\n",
      "2484 images\n"
     ]
    }
   ],
   "source": [
    "dataset17_train = DAVIS17Offline(train=True, mini=False, mega=False, db_root_dir=db_root_dir, transform=apply_custom_transform, inputRes=(224,224))\n",
    "dataloader17_train = DataLoader(dataset17_train, batch_size=batch_size, shuffle=True, num_workers=3)\n",
    "\n",
    "dataset17_val = DAVIS17Offline(train=False, mini=False, mega=False, db_root_dir=db_root_dir, transform=apply_val_custom_transform,inputRes=(224,224))\n",
    "dataloader17_val = DataLoader(dataset17_val, batch_size=vbatch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "lr_factor_array = [1,1,1,0.1,1,1,1,0.1,1,1,1,1,1,0.1,1,1,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dataloader17_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training phase\n",
      "len of loader: 9932\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, nEpochs+1):\n",
    "\n",
    "    trainingDataSetSize = 0\n",
    "    epochLoss = 0\n",
    "    epochTrainIOU = 0\n",
    "\n",
    "    temp_Iou=0\n",
    "\n",
    "    valDataSetSize = 0\n",
    "    epochValLoss = 0\n",
    "    epochValIOU = 0\n",
    "\n",
    "\n",
    "    start_time = timeit.default_timer()\n",
    "    epoch_start_time = datetime.datetime.now()\n",
    "\n",
    "    total_train_batch = len(dataloader17_train)\n",
    "    print('Training phase')\n",
    "    print('len of loader: ' + str(total_train_batch))\n",
    "\n",
    "    net.train()\n",
    "    optimizer.zero_grad()\n",
    "    aveGrad = 0\n",
    "\n",
    "    for data_id, sample in enumerate(dataloader17_train):\n",
    "\n",
    "        dic = net.state_dict()\n",
    "\n",
    "        image = sample['image']\n",
    "        anno = sample['gt']\n",
    "        deformation = sample['deformation']\n",
    "\n",
    "        # Making sure the mask input is similar to RGB values\n",
    "        deformation[deformation==0] = -100\n",
    "        deformation[deformation==1] = 100\n",
    "\n",
    "\n",
    "        prev_frame_mask = Variable(deformation).float()\n",
    "        inputs, gts = Variable(image), Variable(anno)\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs, gts, prev_frame_mask = inputs.cuda(), gts.cuda(), prev_frame_mask.cuda()\n",
    "\n",
    "        input_rgb_mask = torch.cat([inputs, prev_frame_mask], 1)\n",
    "        noImages, noChannels, height, width = input_rgb_mask.shape\n",
    "\n",
    "\n",
    "\n",
    "        output_mask = net(input_rgb_mask)\n",
    "\n",
    "        upsampler = torch.nn.Upsample(size=(height, width), mode='bilinear')\n",
    "        output_mask = upsampler(output_mask)\n",
    "\n",
    "        if debug_mode:\n",
    "            temp_out = np.zeros(output_mask[0][0].shape)\n",
    "            temp_out[output_mask.data.cpu().numpy()[0][1] > output_mask.data.cpu().numpy()[0][0]] = 1\n",
    "            cv2.imwrite('output.png',temp_out*255)\n",
    "\n",
    "        loss1 = cross_entropy_loss(output_mask, gts)\n",
    "\n",
    "        Iou_t = calculate_IOU(output_mask, gts)\n",
    "        if data_id==0:\n",
    "            temp_Iou = Iou_t\n",
    "        else:\n",
    "            temp_Iou = temp_Iou*0.999 + Iou_t*0.001\n",
    "        epochTrainIOU += Iou_t\n",
    "        now_time = datetime.datetime.now()\n",
    "        remain_time = (now_time-epoch_start_time)*((total_train_batch-data_id-1)/(data_id+1))\n",
    "        print('{} time remain {} epoch {} {}/{} train loss:{:.5f} Iou:{:.4f} lr:{} aveIou {:.4f}'.format(\n",
    "            now_time,remain_time,epoch,(data_id+1)*batch_size,(total_train_batch)*batch_size,loss1.item(),Iou_t,learnRate,temp_Iou))\n",
    "\n",
    "        loss_minibatch_array.append(loss1.item())\n",
    "\n",
    "        epochLoss += loss1.item()\n",
    "        trainingDataSetSize += 1\n",
    "\n",
    "        # Backward the averaged gradient\n",
    "        loss1 /= nAveGrad\n",
    "        loss1.backward()\n",
    "        aveGrad += 1\n",
    "\n",
    "        # Update the weights once in nAveGrad forward passes\n",
    "        if aveGrad % nAveGrad == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            aveGrad = 0\n",
    "        if (data_id+1)%10000==0:\n",
    "            torch.save(net.state_dict(), os.path.join(save_dir, modelName + '_epoch-' + str(epoch) + '.pth'))\n",
    "            print('stage saved')\n",
    "\n",
    "    epochLoss = epochLoss / trainingDataSetSize\n",
    "    epochTrainIOU = epochTrainIOU / trainingDataSetSize\n",
    "\n",
    "    print('Epoch: ' + str(epoch) + ', Training Loss: ' + str(epochLoss) + '\\n')\n",
    "    print('Epoch: ' + str(epoch) + ', Training IOU: ' + str(epochTrainIOU) + '\\n')\n",
    "\n",
    "    file_offline_loss.write(str(datetime.datetime.now())\n",
    "                            +' Epoch: ' + str(epoch)\n",
    "                            +', Loss: ' + str(epochLoss)\n",
    "                            +', IOU: ' +str(epochTrainIOU)\n",
    "                            +', lr: ' +str(learnRate)\n",
    "                            + '\\n')\n",
    "    loss_array.append(epochLoss)\n",
    "\n",
    "    file_offline_loss.flush()\n",
    "    torch.save(net.state_dict(), os.path.join(save_dir, modelName + '_epoch-' + str(epoch) + '.pth'))\n",
    "\n",
    "    print('Validation phase')\n",
    "    total_val_batch = len(dataloader17_val)\n",
    "    aveGrad = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for data_id, sample in enumerate(dataloader17_val):\n",
    "\n",
    "            image = sample['image']\n",
    "            anno = sample['gt']\n",
    "            deformation = sample['deformation']\n",
    "\n",
    "            deformation[deformation==0] = -100\n",
    "            deformation[deformation==1] = 100\n",
    "\n",
    "            prev_frame_mask = Variable(deformation, volatile=True).float()\n",
    "            inputs, gts = Variable(image, volatile=True), Variable(anno, volatile=True)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, gts, prev_frame_mask = inputs.cuda(), gts.cuda(), prev_frame_mask.cuda()\n",
    "\n",
    "            input_rgb_mask = torch.cat([inputs, prev_frame_mask], 1)\n",
    "\n",
    "            noImages, noChannels, height, width = input_rgb_mask.shape\n",
    "\n",
    "            output_mask = net(input_rgb_mask)\n",
    "\n",
    "            upsampler = torch.nn.Upsample(size=(height, width), mode='bilinear')\n",
    "            output_mask = upsampler(output_mask)\n",
    "\n",
    "\n",
    "            loss1 = cross_entropy_loss(output_mask, gts)\n",
    "\n",
    "            Iou_t = calculate_IOU(output_mask, gts)\n",
    "            epochValIOU += Iou_t\n",
    "            print('{} epoch {}  {}/{} val loss:{:5f} Iou:{:5f} lr:{}'.format(datetime.datetime.now(),epoch,(data_id+1)*vbatch_size,(total_val_batch)*vbatch_size,loss1.item(),Iou_t,learnRate))\n",
    "\n",
    "            epochValLoss += loss1.item()\n",
    "            valDataSetSize += 1\n",
    "\n",
    "\n",
    "\n",
    "    epochValLoss = epochValLoss / valDataSetSize\n",
    "    epochValIOU = epochValIOU / valDataSetSize\n",
    "\n",
    "\n",
    "\n",
    "    print('Epoch: ' + str(epoch) + ', Val Loss: ' + str(epochValLoss) + '\\n')\n",
    "    print('Epoch: ' + str(epoch) + ', Val IOU: ' + str(epochValIOU) + '\\n')\n",
    "\n",
    "\n",
    "    file_offline_val_loss.write(str(datetime.datetime.now())\n",
    "                                 +' Epoch: ' + str(epoch)\n",
    "                                 + ', Loss: ' + str(epochValLoss) \n",
    "                                 +', IOU: ' +str(epochValIOU)\n",
    "                                 +', lr: ' +str(learnRate)\n",
    "                                 + '\\n')\n",
    "\n",
    "    loss_val_array.append(epochValLoss)\n",
    "\n",
    "    file_offline_val_loss.flush()\n",
    "\n",
    "    epochLoss = 0\n",
    "\n",
    "\n",
    "    stop_time = timeit.default_timer()\n",
    "\n",
    "    epoch_secs = stop_time - start_time\n",
    "    epoch_mins = epoch_secs / 60\n",
    "    epoch_hr = epoch_mins / 60\n",
    "\n",
    "    print('This epoch took: ' + str(epoch_hr) + ' hours')\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr']*lr_factor_array[epoch-1]\n",
    "    learnRate = learnRate*lr_factor_array[epoch-1]\n",
    "\n",
    "\n",
    "    plot_loss1(loss_array, resume_epoch, epoch , save_dir)\n",
    "    plot_loss1(loss_val_array, resume_epoch, epoch , save_dir, val=True)\n",
    "    plot_loss_minibatch(loss_minibatch_array, save_dir)\n",
    "\n",
    "file_offline_loss.close()\n",
    "\n",
    "file_offline_val_loss.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Kjaf5Xedr58"
   },
   "source": [
    "## Task 3 - Online Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1HrOa0Jddr58"
   },
   "outputs": [],
   "source": [
    "# Load saved model from Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0irGmQm5dr58"
   },
   "outputs": [],
   "source": [
    "# Write APIs to perform online training for given test data in next few cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7JDwDewWdr58"
   },
   "outputs": [],
   "source": [
    "# Implement a top level function for inference on test data\n",
    "# Input 1: Path to directory containing 'N' ordered images for given test sample\n",
    "# Input 2: Path to directory containing corresponding 'N' masks with filename as that of images \n",
    "#          Here, use only mask[0] with images for online training where as mask[1:N] are ground truth for evaluating predictions\n",
    "# Output 1: Quality metric for every single mask prediction from time (t=1 to t = T) and average for the same\n",
    "# Output 2: Display all predicted masks along with original RGB image and ground truth masks in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E70PGEBkdr58"
   },
   "source": [
    "## Variants - Extra Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niyk0eK9dr5-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VAR_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
